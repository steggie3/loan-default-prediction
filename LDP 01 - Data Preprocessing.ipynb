{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Default Prediction - Part 01 - Data Preprocessing\n",
    "\n",
    "This is an exploratory project for me to apply what I have learned in the Data Science and Machine Learning courses that I took this year. The data is from a Kaggle competition [Loan Default Prediction](https://www.kaggle.com/c/loan-default-prediction). \n",
    "\n",
    "This is Part 01 of the project: Data Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: A First Glance\n",
    "\n",
    "The data has one __id__ column, 769 feature columns named __f1__ ... __f778__, and one target column __loss__ indicating the percentage of loan unpaid for. There are 105,471 rows. I am using the training data only because __loss__ is not included in the testing data provided on the Kaggle page. \n",
    "\n",
    "The contest organizer is cautious about data privacy and anonymity because the data was collected from real people. As a result, there is no description or explanation of what each feature column means. It's a double-edged sword -- I won't be able to take advantage of the nature of the data, but I also won't be liberally applying heuristics that I _think_ will be helpful but actually have limited use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (135,204,274,417) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data from the CSV file\n",
    "\n",
    "file = 'train_v2.csv'\n",
    "df_raw = pd.read_csv(file, na_values='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 771 entries, id to loss\n",
      "dtypes: float64(653), int64(99), object(19)\n",
      "memory usage: 620.4+ MB\n",
      "                   id             f1             f2             f3  \\\n",
      "count   105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean     52736.000000     134.603171       8.246883       0.499066   \n",
      "std      30446.999458      14.725467       1.691535       0.288752   \n",
      "min          1.000000     103.000000       1.000000       0.000006   \n",
      "25%      26368.500000     124.000000       8.000000       0.248950   \n",
      "50%      52736.000000     129.000000       9.000000       0.498267   \n",
      "75%      79103.500000     148.000000       9.000000       0.749494   \n",
      "max     105471.000000     176.000000      11.000000       0.999994   \n",
      "\n",
      "                   f4             f5             f6             f7  \\\n",
      "count   105471.000000  105471.000000  105471.000000  105289.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean      2678.488874       7.354533   47993.704317    2974.336018   \n",
      "std       1401.010943       5.151112   35677.136048    2546.551085   \n",
      "min       1100.000000       1.000000       0.000000       1.000000   \n",
      "25%       1500.000000       4.000000   11255.000000     629.000000   \n",
      "50%       2200.000000       4.000000   76530.000000    2292.000000   \n",
      "75%       3700.000000      10.000000   80135.000000    4679.000000   \n",
      "max       7900.000000      17.000000   88565.000000    9968.000000   \n",
      "\n",
      "                   f8             f9      ...                 f770  \\\n",
      "count   105370.000000  105471.000000      ...        105471.000000   \n",
      "unique            NaN            NaN      ...                  NaN   \n",
      "top               NaN            NaN      ...                  NaN   \n",
      "freq              NaN            NaN      ...                  NaN   \n",
      "mean      2436.363718     134.555225      ...            17.422543   \n",
      "std       2262.950221      13.824682      ...            18.548936   \n",
      "min          1.000000     106.820000      ...             2.000000   \n",
      "25%        746.000000     124.290000      ...             5.000000   \n",
      "50%       1786.000000     128.460000      ...            11.000000   \n",
      "75%       3411.000000     149.080000      ...            23.000000   \n",
      "max      11541.000000     172.950000      ...           168.000000   \n",
      "\n",
      "                 f771           f772           f773           f774  \\\n",
      "count   105471.000000  105471.000000  105471.000000  104407.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         5.800976      -4.246788       3.273059       0.233852   \n",
      "std          6.508555       4.828265       3.766746       0.073578   \n",
      "min          0.000000     -43.160000       0.000000       0.000000   \n",
      "25%          1.480000      -5.700000       0.740000       0.198400   \n",
      "50%          3.570000      -2.600000       1.990000       0.251800   \n",
      "75%          7.700000      -1.010000       4.440000       0.283600   \n",
      "max         58.120000       0.000000      34.040000       0.473700   \n",
      "\n",
      "                 f775           f776           f777           f778  \\\n",
      "count   103946.000000  105471.000000  105471.000000  105471.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.014797       0.310246       0.322847     175.951589   \n",
      "std          1.039439       0.462597       0.467567     298.294043   \n",
      "min        -18.439600       0.000000       0.000000       2.000000   \n",
      "25%         -0.704275       0.000000       0.000000      19.000000   \n",
      "50%          0.375400       0.000000       0.000000      40.000000   \n",
      "75%          0.737100       1.000000       1.000000     104.000000   \n",
      "max         11.092000       1.000000       1.000000    1212.000000   \n",
      "\n",
      "                 loss  \n",
      "count   105471.000000  \n",
      "unique            NaN  \n",
      "top               NaN  \n",
      "freq              NaN  \n",
      "mean         0.799585  \n",
      "std          4.321120  \n",
      "min          0.000000  \n",
      "25%          0.000000  \n",
      "50%          0.000000  \n",
      "75%          0.000000  \n",
      "max        100.000000  \n",
      "\n",
      "[11 rows x 771 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display basic information\n",
    "\n",
    "df_raw.info()\n",
    "print(df_raw.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the CSV file, I got some warnings about some columns having mixed types. As they may be more tricky to process, I decide to drop them for now. After dropping them, the number of columns reduces from 771 to 751."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop:  ['f137', 'f138', 'f206', 'f207', 'f276', 'f277', 'f338', 'f390', 'f391', 'f419', 'f420', 'f469', 'f472', 'f534', 'f537', 'f626', 'f627', 'f695', 'f698']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 752 entries, id to loss\n",
      "dtypes: float64(653), int64(99)\n",
      "memory usage: 605.1 MB\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = []\n",
    "\n",
    "# Identifying non-numerical features    \n",
    "for c in df_raw.columns:\n",
    "    if (df_raw[c].dtypes == object):\n",
    "        cols_to_drop.append(c)\n",
    "            \n",
    "print(\"Columns to drop: \", cols_to_drop)\n",
    "\n",
    "df_raw = df_raw.drop(cols_to_drop, axis=1)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Data\n",
    "\n",
    "Some columns contain missing values. I iterate over the columns to find which columns have missing values and how many entries they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "{   'f100': 1704,\n",
      "    'f101': 1704,\n",
      "    'f102': 335,\n",
      "    'f103': 335,\n",
      "    'f104': 182,\n",
      "    'f105': 182,\n",
      "    'f106': 182,\n",
      "    'f107': 182,\n",
      "    'f108': 182,\n",
      "    'f109': 182,\n",
      "    'f110': 1491,\n",
      "    'f111': 1491,\n",
      "    'f112': 335,\n",
      "    'f113': 335,\n",
      "    'f114': 182,\n",
      "    'f115': 182,\n",
      "    'f116': 182,\n",
      "    'f117': 182,\n",
      "    'f118': 182,\n",
      "    'f119': 182,\n",
      "    'f120': 1121,\n",
      "    'f121': 1121,\n",
      "    'f122': 335,\n",
      "    'f123': 335,\n",
      "    'f124': 182,\n",
      "    'f125': 182,\n",
      "    'f126': 182,\n",
      "    'f127': 182,\n",
      "    'f128': 182,\n",
      "    'f129': 182,\n",
      "    'f130': 808,\n",
      "    'f131': 808,\n",
      "    'f132': 335,\n",
      "    'f133': 335,\n",
      "    'f134': 182,\n",
      "    'f135': 182,\n",
      "    'f136': 182,\n",
      "    'f139': 182,\n",
      "    'f14': 100,\n",
      "    'f140': 682,\n",
      "    'f141': 682,\n",
      "    'f142': 2561,\n",
      "    'f143': 2561,\n",
      "    'f144': 1291,\n",
      "    'f145': 1291,\n",
      "    'f146': 1291,\n",
      "    'f147': 1291,\n",
      "    'f148': 1291,\n",
      "    'f149': 2859,\n",
      "    'f15': 48,\n",
      "    'f150': 2859,\n",
      "    'f151': 2561,\n",
      "    'f152': 2561,\n",
      "    'f153': 1291,\n",
      "    'f154': 1291,\n",
      "    'f155': 1291,\n",
      "    'f156': 1291,\n",
      "    'f157': 1291,\n",
      "    'f158': 1291,\n",
      "    'f159': 18736,\n",
      "    'f160': 18736,\n",
      "    'f161': 2561,\n",
      "    'f162': 2561,\n",
      "    'f163': 1291,\n",
      "    'f164': 1291,\n",
      "    'f165': 1291,\n",
      "    'f166': 1291,\n",
      "    'f167': 1291,\n",
      "    'f168': 1291,\n",
      "    'f169': 18417,\n",
      "    'f17': 159,\n",
      "    'f170': 18417,\n",
      "    'f171': 2561,\n",
      "    'f172': 2561,\n",
      "    'f173': 1291,\n",
      "    'f174': 1291,\n",
      "    'f175': 1291,\n",
      "    'f176': 1291,\n",
      "    'f177': 1291,\n",
      "    'f178': 1291,\n",
      "    'f179': 17162,\n",
      "    'f18': 23,\n",
      "    'f180': 17162,\n",
      "    'f181': 2561,\n",
      "    'f182': 2561,\n",
      "    'f183': 1291,\n",
      "    'f184': 1291,\n",
      "    'f185': 1291,\n",
      "    'f186': 1291,\n",
      "    'f187': 1291,\n",
      "    'f188': 1291,\n",
      "    'f189': 12234,\n",
      "    'f19': 23,\n",
      "    'f190': 12234,\n",
      "    'f191': 2561,\n",
      "    'f192': 2561,\n",
      "    'f193': 1291,\n",
      "    'f194': 1291,\n",
      "    'f195': 1291,\n",
      "    'f196': 1291,\n",
      "    'f197': 1291,\n",
      "    'f198': 1291,\n",
      "    'f199': 9068,\n",
      "    'f20': 460,\n",
      "    'f200': 9068,\n",
      "    'f201': 2561,\n",
      "    'f202': 2561,\n",
      "    'f203': 1291,\n",
      "    'f204': 1291,\n",
      "    'f205': 1291,\n",
      "    'f208': 1291,\n",
      "    'f209': 6861,\n",
      "    'f21': 1840,\n",
      "    'f210': 6861,\n",
      "    'f211': 174,\n",
      "    'f212': 174,\n",
      "    'f213': 101,\n",
      "    'f214': 101,\n",
      "    'f215': 101,\n",
      "    'f216': 101,\n",
      "    'f217': 101,\n",
      "    'f218': 101,\n",
      "    'f219': 182,\n",
      "    'f22': 1698,\n",
      "    'f220': 182,\n",
      "    'f221': 174,\n",
      "    'f222': 174,\n",
      "    'f223': 101,\n",
      "    'f224': 101,\n",
      "    'f225': 101,\n",
      "    'f226': 101,\n",
      "    'f227': 101,\n",
      "    'f228': 101,\n",
      "    'f229': 1104,\n",
      "    'f23': 698,\n",
      "    'f230': 1104,\n",
      "    'f231': 174,\n",
      "    'f232': 174,\n",
      "    'f233': 101,\n",
      "    'f234': 101,\n",
      "    'f235': 101,\n",
      "    'f236': 101,\n",
      "    'f237': 101,\n",
      "    'f238': 101,\n",
      "    'f239': 992,\n",
      "    'f240': 992,\n",
      "    'f241': 174,\n",
      "    'f242': 174,\n",
      "    'f243': 101,\n",
      "    'f244': 101,\n",
      "    'f245': 101,\n",
      "    'f246': 101,\n",
      "    'f247': 101,\n",
      "    'f248': 101,\n",
      "    'f249': 790,\n",
      "    'f250': 790,\n",
      "    'f251': 174,\n",
      "    'f252': 174,\n",
      "    'f253': 101,\n",
      "    'f254': 101,\n",
      "    'f255': 101,\n",
      "    'f256': 101,\n",
      "    'f257': 101,\n",
      "    'f258': 101,\n",
      "    'f259': 682,\n",
      "    'f26': 698,\n",
      "    'f260': 682,\n",
      "    'f261': 174,\n",
      "    'f262': 174,\n",
      "    'f263': 101,\n",
      "    'f264': 101,\n",
      "    'f265': 101,\n",
      "    'f266': 101,\n",
      "    'f267': 101,\n",
      "    'f268': 101,\n",
      "    'f269': 458,\n",
      "    'f270': 458,\n",
      "    'f271': 174,\n",
      "    'f272': 174,\n",
      "    'f273': 101,\n",
      "    'f274': 101,\n",
      "    'f275': 101,\n",
      "    'f278': 101,\n",
      "    'f279': 348,\n",
      "    'f280': 348,\n",
      "    'f289': 1072,\n",
      "    'f31': 698,\n",
      "    'f32': 2572,\n",
      "    'f330': 18067,\n",
      "    'f331': 18067,\n",
      "    'f340': 11911,\n",
      "    'f341': 11911,\n",
      "    'f348': 5,\n",
      "    'f349': 933,\n",
      "    'f350': 933,\n",
      "    'f356': 5,\n",
      "    'f357': 933,\n",
      "    'f358': 933,\n",
      "    'f365': 10,\n",
      "    'f366': 796,\n",
      "    'f367': 796,\n",
      "    'f373': 13,\n",
      "    'f374': 801,\n",
      "    'f375': 801,\n",
      "    'f384': 726,\n",
      "    'f385': 726,\n",
      "    'f386': 1523,\n",
      "    'f387': 1523,\n",
      "    'f388': 698,\n",
      "    'f389': 698,\n",
      "    'f39': 54,\n",
      "    'f392': 698,\n",
      "    'f393': 2412,\n",
      "    'f394': 2412,\n",
      "    'f397': 9,\n",
      "    'f398': 1650,\n",
      "    'f399': 1631,\n",
      "    'f40': 54,\n",
      "    'f401': 698,\n",
      "    'f402': 96,\n",
      "    'f41': 2,\n",
      "    'f412': 379,\n",
      "    'f413': 379,\n",
      "    'f42': 2,\n",
      "    'f422': 14235,\n",
      "    'f43': 234,\n",
      "    'f432': 1072,\n",
      "    'f433': 1072,\n",
      "    'f434': 335,\n",
      "    'f435': 335,\n",
      "    'f436': 182,\n",
      "    'f437': 182,\n",
      "    'f438': 182,\n",
      "    'f439': 182,\n",
      "    'f44': 963,\n",
      "    'f440': 182,\n",
      "    'f441': 182,\n",
      "    'f442': 201,\n",
      "    'f443': 201,\n",
      "    'f444': 104,\n",
      "    'f448': 136,\n",
      "    'f45': 2078,\n",
      "    'f451': 136,\n",
      "    'f452': 335,\n",
      "    'f453': 335,\n",
      "    'f454': 182,\n",
      "    'f455': 182,\n",
      "    'f456': 182,\n",
      "    'f457': 182,\n",
      "    'f458': 1757,\n",
      "    'f459': 182,\n",
      "    'f46': 2078,\n",
      "    'f460': 182,\n",
      "    'f461': 182,\n",
      "    'f468': 721,\n",
      "    'f47': 853,\n",
      "    'f471': 721,\n",
      "    'f476': 182,\n",
      "    'f479': 772,\n",
      "    'f48': 853,\n",
      "    'f481': 182,\n",
      "    'f482': 182,\n",
      "    'f483': 335,\n",
      "    'f484': 335,\n",
      "    'f485': 182,\n",
      "    'f486': 182,\n",
      "    'f487': 182,\n",
      "    'f488': 182,\n",
      "    'f489': 1157,\n",
      "    'f49': 98,\n",
      "    'f490': 182,\n",
      "    'f491': 182,\n",
      "    'f492': 182,\n",
      "    'f493': 335,\n",
      "    'f494': 335,\n",
      "    'f495': 182,\n",
      "    'f496': 182,\n",
      "    'f497': 182,\n",
      "    'f498': 182,\n",
      "    'f499': 1806,\n",
      "    'f50': 98,\n",
      "    'f500': 182,\n",
      "    'f501': 182,\n",
      "    'f502': 182,\n",
      "    'f503': 335,\n",
      "    'f504': 335,\n",
      "    'f505': 182,\n",
      "    'f506': 182,\n",
      "    'f507': 182,\n",
      "    'f508': 182,\n",
      "    'f509': 1534,\n",
      "    'f51': 2,\n",
      "    'f510': 182,\n",
      "    'f511': 182,\n",
      "    'f512': 182,\n",
      "    'f513': 23,\n",
      "    'f514': 1523,\n",
      "    'f516': 121,\n",
      "    'f517': 121,\n",
      "    'f518': 46,\n",
      "    'f52': 2,\n",
      "    'f522': 80,\n",
      "    'f525': 80,\n",
      "    'f527': 101,\n",
      "    'f528': 101,\n",
      "    'f53': 220,\n",
      "    'f533': 378,\n",
      "    'f536': 378,\n",
      "    'f538': 174,\n",
      "    'f539': 174,\n",
      "    'f54': 950,\n",
      "    'f540': 101,\n",
      "    'f541': 101,\n",
      "    'f542': 101,\n",
      "    'f543': 101,\n",
      "    'f544': 101,\n",
      "    'f545': 101,\n",
      "    'f546': 1160,\n",
      "    'f547': 101,\n",
      "    'f548': 174,\n",
      "    'f549': 174,\n",
      "    'f55': 54,\n",
      "    'f550': 101,\n",
      "    'f551': 101,\n",
      "    'f552': 101,\n",
      "    'f553': 101,\n",
      "    'f554': 101,\n",
      "    'f555': 101,\n",
      "    'f556': 1108,\n",
      "    'f557': 101,\n",
      "    'f558': 174,\n",
      "    'f559': 174,\n",
      "    'f56': 54,\n",
      "    'f560': 101,\n",
      "    'f561': 101,\n",
      "    'f562': 101,\n",
      "    'f563': 101,\n",
      "    'f564': 101,\n",
      "    'f565': 101,\n",
      "    'f566': 807,\n",
      "    'f567': 101,\n",
      "    'f568': 174,\n",
      "    'f569': 174,\n",
      "    'f57': 2,\n",
      "    'f570': 101,\n",
      "    'f571': 101,\n",
      "    'f572': 101,\n",
      "    'f573': 101,\n",
      "    'f574': 101,\n",
      "    'f575': 101,\n",
      "    'f576': 101,\n",
      "    'f577': 174,\n",
      "    'f578': 174,\n",
      "    'f579': 101,\n",
      "    'f58': 2,\n",
      "    'f580': 101,\n",
      "    'f581': 101,\n",
      "    'f582': 101,\n",
      "    'f583': 101,\n",
      "    'f584': 101,\n",
      "    'f585': 101,\n",
      "    'f586': 8965,\n",
      "    'f587': 8965,\n",
      "    'f588': 8432,\n",
      "    'f59': 1879,\n",
      "    'f60': 1879,\n",
      "    'f600': 366,\n",
      "    'f601': 366,\n",
      "    'f604': 1580,\n",
      "    'f606': 201,\n",
      "    'f607': 201,\n",
      "    'f608': 201,\n",
      "    'f609': 370,\n",
      "    'f61': 800,\n",
      "    'f610': 201,\n",
      "    'f611': 201,\n",
      "    'f612': 370,\n",
      "    'f613': 210,\n",
      "    'f614': 210,\n",
      "    'f615': 1149,\n",
      "    'f616': 844,\n",
      "    'f617': 1149,\n",
      "    'f618': 18407,\n",
      "    'f619': 18407,\n",
      "    'f62': 800,\n",
      "    'f620': 8180,\n",
      "    'f621': 8180,\n",
      "    'f622': 1523,\n",
      "    'f623': 1523,\n",
      "    'f624': 698,\n",
      "    'f625': 698,\n",
      "    'f628': 698,\n",
      "    'f629': 323,\n",
      "    'f63': 1154,\n",
      "    'f630': 323,\n",
      "    'f631': 96,\n",
      "    'f632': 96,\n",
      "    'f633': 96,\n",
      "    'f634': 96,\n",
      "    'f635': 96,\n",
      "    'f636': 96,\n",
      "    'f637': 562,\n",
      "    'f638': 818,\n",
      "    'f639': 818,\n",
      "    'f64': 1154,\n",
      "    'f640': 9700,\n",
      "    'f641': 54,\n",
      "    'f642': 54,\n",
      "    'f643': 2,\n",
      "    'f644': 2,\n",
      "    'f645': 88,\n",
      "    'f646': 830,\n",
      "    'f647': 830,\n",
      "    'f648': 8657,\n",
      "    'f649': 8716,\n",
      "    'f65': 158,\n",
      "    'f650': 9003,\n",
      "    'f651': 9003,\n",
      "    'f652': 2,\n",
      "    'f653': 13205,\n",
      "    'f654': 4,\n",
      "    'f655': 4,\n",
      "    'f66': 649,\n",
      "    'f660': 120,\n",
      "    'f661': 120,\n",
      "    'f662': 18833,\n",
      "    'f663': 18833,\n",
      "    'f664': 11282,\n",
      "    'f665': 11282,\n",
      "    'f666': 11282,\n",
      "    'f667': 11282,\n",
      "    'f668': 11282,\n",
      "    'f669': 11282,\n",
      "    'f671': 9,\n",
      "    'f672': 7343,\n",
      "    'f673': 7343,\n",
      "    'f675': 1,\n",
      "    'f676': 1,\n",
      "    'f677': 1,\n",
      "    'f678': 1,\n",
      "    'f679': 6393,\n",
      "    'f694': 698,\n",
      "    'f696': 59,\n",
      "    'f697': 698,\n",
      "    'f7': 182,\n",
      "    'f703': 2,\n",
      "    'f704': 2,\n",
      "    'f705': 2,\n",
      "    'f706': 853,\n",
      "    'f707': 2,\n",
      "    'f708': 2,\n",
      "    'f709': 2,\n",
      "    'f710': 2,\n",
      "    'f711': 800,\n",
      "    'f712': 158,\n",
      "    'f713': 158,\n",
      "    'f714': 158,\n",
      "    'f716': 304,\n",
      "    'f717': 201,\n",
      "    'f718': 2,\n",
      "    'f719': 2,\n",
      "    'f72': 9002,\n",
      "    'f720': 2,\n",
      "    'f721': 2,\n",
      "    'f726': 11282,\n",
      "    'f735': 182,\n",
      "    'f746': 1432,\n",
      "    'f747': 2,\n",
      "    'f748': 853,\n",
      "    'f749': 853,\n",
      "    'f75': 335,\n",
      "    'f750': 853,\n",
      "    'f751': 1458,\n",
      "    'f752': 2,\n",
      "    'f753': 2,\n",
      "    'f754': 2,\n",
      "    'f755': 2,\n",
      "    'f756': 233,\n",
      "    'f757': 800,\n",
      "    'f758': 800,\n",
      "    'f759': 800,\n",
      "    'f76': 335,\n",
      "    'f760': 1334,\n",
      "    'f761': 158,\n",
      "    'f762': 158,\n",
      "    'f763': 1840,\n",
      "    'f765': 1,\n",
      "    'f77': 182,\n",
      "    'f774': 1064,\n",
      "    'f775': 1525,\n",
      "    'f78': 182,\n",
      "    'f79': 182,\n",
      "    'f8': 101,\n",
      "    'f80': 346,\n",
      "    'f81': 346,\n",
      "    'f82': 335,\n",
      "    'f83': 335,\n",
      "    'f84': 182,\n",
      "    'f85': 182,\n",
      "    'f86': 182,\n",
      "    'f87': 182,\n",
      "    'f88': 182,\n",
      "    'f89': 182,\n",
      "    'f90': 1719,\n",
      "    'f91': 1719,\n",
      "    'f92': 335,\n",
      "    'f93': 335,\n",
      "    'f94': 182,\n",
      "    'f95': 182,\n",
      "    'f96': 182,\n",
      "    'f97': 182,\n",
      "    'f98': 182,\n",
      "    'f99': 182}\n"
     ]
    }
   ],
   "source": [
    "# Look for columns with missing values\n",
    "\n",
    "cols_with_missing_vals = dict()\n",
    "for c in df_raw.columns:\n",
    "    if df_raw[c].isnull().sum() > 0:\n",
    "        cols_with_missing_vals[c] = df_raw[c].isnull().sum()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(\"Columns with missing values:\")\n",
    "pp.pprint(cols_with_missing_vals)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there are many columns with missing values. I use [sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) to fill the missing values. I choose the _most frequent_ imputation stratety because I believe some of the columns may be categorical features, and _mean_ or _median_ wouldn't make much sense.\n",
    "\n",
    "I apply the Imputer using the [sklearn-pandas](https://github.com/scikit-learn-contrib/sklearn-pandas) DataFrameMapper framework so that the transformed data stays as a DataFrame. After imputing the data, there are no more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 752 entries, f7 to loss\n",
      "dtypes: float64(752)\n",
      "memory usage: 605.1 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn_pandas import gen_features\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "cols_to_impute = []\n",
    "for key, value in cols_with_missing_vals.items():\n",
    "    cols_to_impute.append([key])\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns=cols_to_impute,\n",
    "    classes=[{'class': Imputer, 'strategy': 'most_frequent'}]\n",
    ")\n",
    "\n",
    "mapper = DataFrameMapper(feature_def, default=None, input_df=True, df_out=True)\n",
    "df_imputed = mapper.fit_transform(df_raw)\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing_vals = dict()\n",
    "for c in df_imputed.columns:\n",
    "    if df_imputed[c].isnull().sum() > 0:\n",
    "        cols_with_missing_vals[c] = df_imputed[c].isnull().sum()\n",
    "        \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(\"Columns with missing values:\")\n",
    "pp.pprint(cols_with_missing_vals)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding for Categorical Features\n",
    "\n",
    "As the values of categorical features have no physical meaning, they are usually transformed into one-hot encoding before being used to train any models. For this particular dataset, a tricky part is that we don't know which features are categorical. I can only assume that a column is likely to be categorical if (1) it is entirely composed of integer values and (2) the number of unique values is small (i.e. under a certain threshold value). I apply this rule to the columns to determine which columns are categorical, and then I use [sklearn.preprocessing.OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to transform these columns to one-hot encoding. \n",
    "\n",
    "Using the DataFrameMapper framework, the new one-hot-encoded columns are systematically named. For example, column __f1__ has 74 unique values. After the tranformation, __f1__ is removed, and 74 new columns __f1_0__ ... __f1_73__ are added.\n",
    "\n",
    "In addition, if any column contains only one value, it has no information, and we can safely drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \"f1\" has unique values: \n",
      "[126 121 134 109 128 127 115 120 130 119 116 123 125 114 129 117 124 122\n",
      " 139 144 142 137 154 150 147 156 157 146 153 135 138 118 113 133 110 111\n",
      " 136 132 108 106 152 145 149 155 163 166 165 160 148 159 151 162 158 131\n",
      " 143 161 164 140 141 173 167 112 168 107 170 105 171 103 104 169 174 176\n",
      " 172 175]\n",
      "Column \"f2\" has unique values: \n",
      "[10  9  8  6  7  4  2  3  1 11]\n",
      "Column \"f4\" has unique values: \n",
      "[1100 2900 1300 1800 2200 3800 2600 1600 3500 1900 2700 4800 4200 3600\n",
      " 3700 1500 2400 4300 5400 2000 3100 5500 5000 4400 4700 2300 5100 5300\n",
      " 1200 3900 6200 5900 5700 4100 4000 4500 1700 2500 6600 3300 2100 3000\n",
      " 5800 3200 2800 4600 5600 7200 3400 1400 5200 6100 6800 7000 6500 6300\n",
      " 6400 7300 6700 7400 7900]\n",
      "Column \"f5\" has unique values: \n",
      "[ 3  4 16 10  2  7 15  1 17 13]\n",
      "Column \"f13\" has unique values: \n",
      "[ 7  6 14 13 15  8 10 11  4 16 12  5  9  3 17 18 20 19 25 29 28 27 23 26\n",
      " 21 24 22 32  2 40 30]\n",
      "Column \"f33\" has unique values: \n",
      "[0]\n",
      "Column \"f34\" has unique values: \n",
      "[0]\n",
      "Column \"f35\" has unique values: \n",
      "[0]\n",
      "Column \"f37\" has unique values: \n",
      "[0]\n",
      "Column \"f38\" has unique values: \n",
      "[0]\n",
      "Column \"f68\" has unique values: \n",
      "[ 6 14 13 15  7 10  8 11  3  2 16 12  5  9  4 17  1 18 20 19 24 29 26 28\n",
      " 23 25 27 21 22 30  0]\n",
      "Column \"f73\" has unique values: \n",
      "[ 9  8  6 10  7  4  3  1 11  2]\n",
      "Column \"f293\" has unique values: \n",
      "[ 0  4  3  1 13  8 10  9  5  2  7 12  6 11 15 14 20 17 23 16 19 18]\n",
      "Column \"f294\" has unique values: \n",
      "[ 0  4  3  1 13  8 10  9  5  2  7 12  6 11 15 14 20 17 23 16 19 18]\n",
      "Column \"f295\" has unique values: \n",
      "[ 0  4  3  1 13  8 10  9  5  2  7 12  6 11 15 14 20 17 23 16 19 18]\n",
      "Column \"f296\" has unique values: \n",
      "[ 0  4  3  1 13  8 10  9  5  2  7 12  6 11 15 14 20 17 23 16 19 18]\n",
      "Column \"f301\" has unique values: \n",
      "[ 2  0  1  4 11  7  5  9  6  8  3 12 18 14 10 13 17 15 16 19 23 24 20 25\n",
      " 21]\n",
      "Column \"f302\" has unique values: \n",
      "[ 2  0  1  4 11  7  5  9  6  8  3 12 18 14 10 13 17 15 16 19 23 24 20 25\n",
      " 21]\n",
      "Column \"f303\" has unique values: \n",
      "[ 2  0  1  4 11  7  5  9  6  8  3 12 18 14 10 13 17 15 16 19 23 24 20 25\n",
      " 21]\n",
      "Column \"f304\" has unique values: \n",
      "[ 2  0  1  4 11  7  5  9  6  8  3 12 18 14 10 13 17 15 16 19 23 24 20 25\n",
      " 21]\n",
      "Column \"f309\" has unique values: \n",
      "[ 3  0  1  2  7 11 12  8  4 10  9  6 13  5 14 15 17 18 24 19 16 25 26 20]\n",
      "Column \"f310\" has unique values: \n",
      "[ 3  0  1  2  7 11 12  8  4 10  9  6 13  5 14 15 17 18 24 19 16 25 26 20]\n",
      "Column \"f311\" has unique values: \n",
      "[ 3  0  1  2  7 11 12  8  4 10  9  6 13  5 14 15 17 18 24 19 16 25 26 20]\n",
      "Column \"f312\" has unique values: \n",
      "[ 3  0  1  2  7 11 12  8  4 10  9  6 13  5 14 15 17 18 24 19 16 25 26 20]\n",
      "Column \"f317\" has unique values: \n",
      "[ 2  0  1  8  4 24 15 21  9 18 14 17  6  3 12  7 19 10 30 16 11  5 25 13\n",
      " 20 37 23 28 22 27 26 39 34 31 29 32 35 33 36 38 40]\n",
      "Column \"f318\" has unique values: \n",
      "[ 2  0  1  8  4 24 15 21  9 18 14 17  6  3 12  7 19 10 30 16 11  5 25 13\n",
      " 20 37 23 28 22 27 26 39 34 31 29 32 35 33 36 38 40]\n",
      "Column \"f319\" has unique values: \n",
      "[ 2  0  1  8  4 24 15 21  9 18 14 17  6  3 12  7 19 10 30 16 11  5 25 13\n",
      " 20 37 23 28 22 27 26 39 34 31 29 32 35 33 36 38 40]\n",
      "Column \"f320\" has unique values: \n",
      "[ 2  0  1  8  4 24 15 21  9 18 14 17  6  3 12  7 19 10 30 16 11  5 25 13\n",
      " 20 37 23 28 22 27 26 39 34 31 29 32 35 33 36 38 40]\n",
      "Column \"f325\" has unique values: \n",
      "[ 5  0  3  2  8  6  4 31 26 23 21 18 25 16  9  7  1 15 19 40 17 13 12 11\n",
      " 14 38 33 32 37 20 10 35 22 24 48 28 27 34 29 41 30 44 36 50 39 42 49 45\n",
      " 51 46 47 43 52 53]\n",
      "Column \"f326\" has unique values: \n",
      "[ 5  0  3  2  8  6  4 31 26 23 21 18 25 16  9  7  1 15 19 40 17 13 12 11\n",
      " 14 38 33 32 37 20 10 35 22 24 48 28 27 34 29 41 30 44 36 50 39 42 49 45\n",
      " 51 46 47 43 52 53]\n",
      "Column \"f327\" has unique values: \n",
      "[ 5  0  3  2  8  6  4 31 26 23 21 18 25 16  9  7  1 15 19 40 17 13 12 11\n",
      " 14 38 33 32 37 20 10 35 22 24 48 28 27 34 29 41 30 44 36 50 39 42 49 45\n",
      " 51 46 47 43 52 53]\n",
      "Column \"f328\" has unique values: \n",
      "[ 5  0  3  2  8  6  4 31 26 23 21 18 25 16  9  7  1 15 19 40 17 13 12 11\n",
      " 14 38 33 32 37 20 10 35 22 24 48 28 27 34 29 41 30 44 36 50 39 42 49 45\n",
      " 51 46 47 43 52 53]\n",
      "Column \"f400\" has unique values: \n",
      "[ 3 10  5  2  9  4  1 11 12  6  8 13  7 15 14 16 25 20 17 18 19 29 26 21\n",
      " 27 24 23 22 28 99 40 30 32 31 33]\n",
      "Column \"f403\" has unique values: \n",
      "[ 3 16  4 17 10  7 15 13  1  2  5 12  6 11 20  9 14 19]\n",
      "Column \"f597\" has unique values: \n",
      "[ 5  6 14 13 15 10  7  8 11  1  2 12 16  4  9  3 17 18 20 19 29 28 27 23\n",
      " 25 26 21 22 32 24 40 30]\n",
      "Column \"f599\" has unique values: \n",
      "[ 5  6 14 13 15 10  7  8 11  1  2 12 16  4  9  3 17 18 20 19 29 28 27 23\n",
      " 25 26 21 22 32 24 40 30]\n",
      "Column \"f700\" has unique values: \n",
      "[0]\n",
      "Column \"f701\" has unique values: \n",
      "[0]\n",
      "Column \"f702\" has unique values: \n",
      "[0]\n",
      "Column \"f725\" has unique values: \n",
      "[0 1 2 3 4 5 6]\n",
      "Column \"f736\" has unique values: \n",
      "[1]\n",
      "Column \"f764\" has unique values: \n",
      "[1]\n",
      "Column \"f776\" has unique values: \n",
      "[1 0]\n",
      "Column \"f777\" has unique values: \n",
      "[0 1]\n",
      "Column \"f778\" has unique values: \n",
      "[   5   16   93  513   39    6   30  394   83   21   54   27   57   23\n",
      "   52   87  101    7   37   38   46   67   10    9   20   84    8   12\n",
      "   47   25   26  104 1083   61    3  107   15  393   31   36   41   80\n",
      "   19   49 1079   22   17   13   40   32   35   85    2   24    4   44\n",
      "   14   90   34   11   28   18   73 1212]\n",
      "Categorical columns:  [['f1'], ['f2'], ['f4'], ['f5'], ['f13'], ['f68'], ['f73'], ['f293'], ['f294'], ['f295'], ['f296'], ['f301'], ['f302'], ['f303'], ['f304'], ['f309'], ['f310'], ['f311'], ['f312'], ['f317'], ['f318'], ['f319'], ['f320'], ['f325'], ['f326'], ['f327'], ['f328'], ['f400'], ['f403'], ['f597'], ['f599'], ['f725'], ['f776'], ['f777'], ['f778']]\n",
      "Single-valued columns:  ['f33', 'f34', 'f35', 'f37', 'f38', 'f700', 'f701', 'f702', 'f736', 'f764']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical features and single-valued features\n",
    "\n",
    "# Define categorical_threshold\n",
    "# If a column's number of unique values is less than this value, consider it categorical\n",
    "categorical_threshold = 100\n",
    "\n",
    "# Identifying categorical features\n",
    "# Using df_raw because df_imputed is all float\n",
    "cols_single_val = []\n",
    "cols_categorical = []\n",
    "cols_continuous = [] # non-categorical\n",
    "for c in df_raw.columns:\n",
    "    if (c == 'id' or c == 'loss'):\n",
    "        continue\n",
    "    if (df_raw[c].dtypes == 'int64' and len(df_raw.loc[:,c].unique()) <= categorical_threshold):\n",
    "        print(\"Column \\\"\" + c + \"\\\" has unique values: \")\n",
    "        print(df_raw.loc[:,c].unique())\n",
    "        if (len(df_raw.loc[:,c].unique()) == 1):\n",
    "            cols_single_val.append(c)\n",
    "        else:\n",
    "            cols_categorical.append([c])\n",
    "    else:\n",
    "        cols_continuous.append([c])\n",
    "\n",
    "# After this check, we no longer need df_raw\n",
    "del df_raw\n",
    "            \n",
    "print(\"Categorical columns: \", cols_categorical)\n",
    "print(\"Single-valued columns: \", cols_single_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 742 entries, f7 to loss\n",
      "dtypes: float64(742)\n",
      "memory usage: 597.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Drop one-value columns\n",
    "\n",
    "df_imputed = df_imputed.drop(cols_single_val, axis=1)\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 1790 entries, f1_0 to loss\n",
      "dtypes: float64(1790)\n",
      "memory usage: 1.4 GB\n",
      "                f1_0           f1_1           f1_2           f1_3  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean        0.000284       0.000076       0.000294       0.000218   \n",
      "std         0.016863       0.008709       0.017142       0.014766   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "                f1_4           f1_5           f1_6           f1_7  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean        0.001185       0.000578       0.002266       0.002579   \n",
      "std         0.034406       0.024042       0.047549       0.050718   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "                f1_8           f1_9      ...                 f745  \\\n",
      "count  105471.000000  105471.000000      ...        105471.000000   \n",
      "mean        0.004124       0.004712      ...             4.192035   \n",
      "std         0.064089       0.068484      ...             2.086577   \n",
      "min         0.000000       0.000000      ...             0.000000   \n",
      "25%         0.000000       0.000000      ...             2.658650   \n",
      "50%         0.000000       0.000000      ...             4.172800   \n",
      "75%         0.000000       0.000000      ...             5.586950   \n",
      "max         1.000000       1.000000      ...            14.909900   \n",
      "\n",
      "                f766           f767           f768           f769  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean       -0.476605      -0.471572      -0.491973      -8.786110   \n",
      "std         0.194983       0.263993       0.141869       9.684043   \n",
      "min        -0.950000      -0.963000      -0.945000     -85.450000   \n",
      "25%        -0.630000      -0.699000      -0.575900     -11.530000   \n",
      "50%        -0.480000      -0.480000      -0.503100      -5.440000   \n",
      "75%        -0.330000      -0.248000      -0.420000      -2.390000   \n",
      "max         0.000000       0.000000       0.000000       0.000000   \n",
      "\n",
      "                f770           f771           f772           f773  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean       17.422543       5.800976      -4.246788       3.273059   \n",
      "std        18.548936       6.508555       4.828265       3.766746   \n",
      "min         2.000000       0.000000     -43.160000       0.000000   \n",
      "25%         5.000000       1.480000      -5.700000       0.740000   \n",
      "50%        11.000000       3.570000      -2.600000       1.990000   \n",
      "75%        23.000000       7.700000      -1.010000       4.440000   \n",
      "max       168.000000      58.120000       0.000000      34.040000   \n",
      "\n",
      "                loss  \n",
      "count  105471.000000  \n",
      "mean        0.799585  \n",
      "std         4.321120  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max       100.000000  \n",
      "\n",
      "[8 rows x 1790 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace categorical features with one-hot encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns=cols_categorical,\n",
    "    classes=[OneHotEncoder] \n",
    ")\n",
    "\n",
    "mapper = DataFrameMapper(feature_def, default=None, input_df=True, df_out=True)\n",
    "df_imputed = mapper.fit_transform(df_imputed)\n",
    "df_imputed.info()\n",
    "print(df_imputed.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the preprocessded DataFrame into a new CSV\n",
    "\n",
    "df_imputed.to_csv('train_v2_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Continuous Features\n",
    "\n",
    "Now that I've preprocessed categorical features, it's time to look at the continuous features (non-categorical). From the printed column information, we can see that the values in these columns are distributed differently. For example, column __f7__ has a range of [1, 9968], while column __f766__ has a range of (-0.5, 0]. The value range can greatly impact the performance of some machine learning models, so I will prepare a normalized version of the DataFrame. I use [sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to scale these features to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 1790 entries, f3 to loss\n",
      "dtypes: float64(1790)\n",
      "memory usage: 1.4 GB\n",
      "                 f3            f6            f7            f8            f9  \\\n",
      "count  1.054710e+05  1.054710e+05  1.054710e+05  1.054710e+05  1.054710e+05   \n",
      "mean   5.622168e-17 -2.554207e-15  5.490115e-17  7.954330e-17  1.151258e-15   \n",
      "std    1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00   \n",
      "min   -1.728340e+00 -1.345229e+00 -1.165221e+00 -1.075084e+00 -2.006220e+00   \n",
      "25%   -8.662007e-01 -1.029760e+00 -9.206516e-01 -7.467748e-01 -7.425324e-01   \n",
      "50%   -2.767762e-03  7.998521e-01 -2.685968e-01 -2.872299e-01 -4.408965e-01   \n",
      "75%    8.672785e-01  9.008977e-01  6.692472e-01  4.305880e-01  1.050646e+00   \n",
      "max    1.734808e+00  1.137185e+00  2.747500e+00  4.024096e+00  2.777276e+00   \n",
      "\n",
      "                f10           f14           f15           f16           f17  \\\n",
      "count  1.054710e+05  1.054710e+05  1.054710e+05  1.054710e+05  1.054710e+05   \n",
      "mean  -2.342023e-15 -2.734250e-15 -3.710438e-15  8.285541e-17  3.960024e-15   \n",
      "std    1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00  1.000005e+00   \n",
      "min   -2.168844e+00 -2.854264e+00 -2.796976e+00 -6.055054e-01 -2.873932e+00   \n",
      "25%   -7.395809e-01 -6.981179e-02 -6.821592e-02 -5.433311e-01 -7.288687e-02   \n",
      "50%   -3.803690e-01  3.079740e-01  3.151226e-01 -3.529443e-01  3.129780e-01   \n",
      "75%    9.454745e-01  5.599680e-01  5.664177e-01  1.088349e-01  5.655207e-01   \n",
      "max    2.804275e+00  1.249873e+00  1.329381e+00  1.002058e+01  1.399211e+00   \n",
      "\n",
      "           ...             f778_56        f778_57        f778_58  \\\n",
      "count      ...        105471.00000  105471.000000  105471.000000   \n",
      "mean       ...             0.01955       0.013900       0.048724   \n",
      "std        ...             0.13845       0.117075       0.215292   \n",
      "min        ...             0.00000       0.000000       0.000000   \n",
      "25%        ...             0.00000       0.000000       0.000000   \n",
      "50%        ...             0.00000       0.000000       0.000000   \n",
      "75%        ...             0.00000       0.000000       0.000000   \n",
      "max        ...             1.00000       1.000000       1.000000   \n",
      "\n",
      "             f778_59        f778_60        f778_61        f778_62  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean        0.040172       0.054517       0.053266       0.022357   \n",
      "std         0.196364       0.227037       0.224564       0.147842   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "             f778_63             id           loss  \n",
      "count  105471.000000  105471.000000  105471.000000  \n",
      "mean        0.001195   52736.000000       0.799585  \n",
      "std         0.034543   30446.999458       4.321120  \n",
      "min         0.000000       1.000000       0.000000  \n",
      "25%         0.000000   26368.500000       0.000000  \n",
      "50%         0.000000   52736.000000       0.000000  \n",
      "75%         0.000000   79103.500000       0.000000  \n",
      "max         1.000000  105471.000000     100.000000  \n",
      "\n",
      "[8 rows x 1790 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns=cols_continuous,\n",
    "    classes=[StandardScaler]\n",
    ")\n",
    "\n",
    "mapper = DataFrameMapper(feature_def, default=None, input_df=True, df_out=True)\n",
    "df_imputed_normalized = mapper.fit_transform(df_imputed)\n",
    "df_imputed_normalized.info()\n",
    "print(df_imputed_normalized.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the normalized DataFrame into a new CSV\n",
    "\n",
    "df_imputed_normalized.to_csv('train_v2_preprocessed_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Expansion\n",
    "\n",
    "The continuous features can also be expanded polynomially, for example, adding x^2 or x^3 terms, to account for some of the nonlinear dependency. I use [sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 3200 entries, f3_1 to loss\n",
      "dtypes: float64(3200)\n",
      "memory usage: 2.5 GB\n",
      "           f3_1         f3_x0       f3_x0^2      f6_1         f6_x0  \\\n",
      "count  105471.0  1.054710e+05  1.054710e+05  105471.0  1.054710e+05   \n",
      "mean        1.0  5.622168e-17  1.000000e+00       1.0 -2.554207e-15   \n",
      "std         0.0  1.000005e+00  8.938727e-01       0.0  1.000005e+00   \n",
      "min         1.0 -1.728340e+00  1.335714e-09       1.0 -1.345229e+00   \n",
      "25%         1.0 -8.662007e-01  1.872665e-01       1.0 -1.029760e+00   \n",
      "50%         1.0 -2.767762e-03  7.511669e-01       1.0  7.998521e-01   \n",
      "75%         1.0  8.672785e-01  1.689483e+00       1.0  9.008977e-01   \n",
      "max         1.0  1.734808e+00  3.009558e+00       1.0  1.137185e+00   \n",
      "\n",
      "             f6_x0^2      f7_1         f7_x0       f7_x0^2      f8_1  \\\n",
      "count  105471.000000  105471.0  1.054710e+05  1.054710e+05  105471.0   \n",
      "mean        1.000000       1.0  5.490115e-17  1.000000e+00       1.0   \n",
      "std         0.311009       0.0  1.000005e+00  1.152921e+00       0.0   \n",
      "min         0.573084       1.0 -1.165221e+00  6.492297e-09       1.0   \n",
      "25%         0.773491       1.0 -9.206516e-01  1.882413e-01       1.0   \n",
      "50%         0.927141       1.0 -2.685968e-01  6.924949e-01       1.0   \n",
      "75%         1.122513       1.0  6.692472e-01  1.250177e+00       1.0   \n",
      "max         1.809642       1.0  2.747500e+00  7.548757e+00       1.0   \n",
      "\n",
      "           ...             f778_56        f778_57        f778_58  \\\n",
      "count      ...        105471.00000  105471.000000  105471.000000   \n",
      "mean       ...             0.01955       0.013900       0.048724   \n",
      "std        ...             0.13845       0.117075       0.215292   \n",
      "min        ...             0.00000       0.000000       0.000000   \n",
      "25%        ...             0.00000       0.000000       0.000000   \n",
      "50%        ...             0.00000       0.000000       0.000000   \n",
      "75%        ...             0.00000       0.000000       0.000000   \n",
      "max        ...             1.00000       1.000000       1.000000   \n",
      "\n",
      "             f778_59        f778_60        f778_61        f778_62  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean        0.040172       0.054517       0.053266       0.022357   \n",
      "std         0.196364       0.227037       0.224564       0.147842   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "             f778_63             id           loss  \n",
      "count  105471.000000  105471.000000  105471.000000  \n",
      "mean        0.001195   52736.000000       0.799585  \n",
      "std         0.034543   30446.999458       4.321120  \n",
      "min         0.000000       1.000000       0.000000  \n",
      "25%         0.000000   26368.500000       0.000000  \n",
      "50%         0.000000   52736.000000       0.000000  \n",
      "75%         0.000000   79103.500000       0.000000  \n",
      "max         1.000000  105471.000000     100.000000  \n",
      "\n",
      "[8 rows x 3200 columns]\n"
     ]
    }
   ],
   "source": [
    "# Expand continuous features to be polynomial (degree 2)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns=cols_continuous,\n",
    "    classes=[{'class': PolynomialFeatures, 'degree': 2}]\n",
    ")\n",
    "\n",
    "mapper = DataFrameMapper(feature_def, default=None, input_df=True, df_out=True)\n",
    "df_poly2 = mapper.fit_transform(df_imputed_normalized)\n",
    "df_poly2.info()\n",
    "print(df_poly2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the expanded DataFrame into a new CSV\n",
    "\n",
    "df_poly2.to_csv('train_v2_preprocessed_normalized_poly2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105471 entries, 0 to 105470\n",
      "Columns: 3905 entries, f3_1 to loss\n",
      "dtypes: float64(3905)\n",
      "memory usage: 3.1 GB\n",
      "           f3_1         f3_x0       f3_x0^2       f3_x0^3      f6_1  \\\n",
      "count  105471.0  1.054710e+05  1.054710e+05  1.054710e+05  105471.0   \n",
      "mean        1.0  5.622168e-17  1.000000e+00  3.568024e-03       1.0   \n",
      "std         0.0  1.000005e+00  8.938727e-01  1.962592e+00       0.0   \n",
      "min         1.0 -1.728340e+00  1.335714e-09 -5.162823e+00       1.0   \n",
      "25%         1.0 -8.662007e-01  1.872665e-01 -6.499135e-01       1.0   \n",
      "50%         1.0 -2.767762e-03  7.511669e-01 -2.120246e-08       1.0   \n",
      "75%         1.0  8.672785e-01  1.689483e+00  6.523427e-01       1.0   \n",
      "max         1.0  1.734808e+00  3.009558e+00  5.221005e+00       1.0   \n",
      "\n",
      "              f6_x0        f6_x0^2        f6_x0^3      f7_1         f7_x0  \\\n",
      "count  1.054710e+05  105471.000000  105471.000000  105471.0  1.054710e+05   \n",
      "mean  -2.554207e-15       1.000000      -0.211704       1.0  5.490115e-17   \n",
      "std    1.000005e+00       0.311009       1.130734       0.0  1.000005e+00   \n",
      "min   -1.345229e+00       0.573084      -2.434384       1.0 -1.165221e+00   \n",
      "25%   -1.029760e+00       0.773491      -1.091963       1.0 -9.206516e-01   \n",
      "50%    7.998521e-01       0.927141       0.511716       1.0 -2.685968e-01   \n",
      "75%    9.008977e-01       1.122513       0.731183       1.0  6.692472e-01   \n",
      "max    1.137185e+00       1.809642       1.470594       1.0  2.747500e+00   \n",
      "\n",
      "           ...             f778_56        f778_57        f778_58  \\\n",
      "count      ...        105471.00000  105471.000000  105471.000000   \n",
      "mean       ...             0.01955       0.013900       0.048724   \n",
      "std        ...             0.13845       0.117075       0.215292   \n",
      "min        ...             0.00000       0.000000       0.000000   \n",
      "25%        ...             0.00000       0.000000       0.000000   \n",
      "50%        ...             0.00000       0.000000       0.000000   \n",
      "75%        ...             0.00000       0.000000       0.000000   \n",
      "max        ...             1.00000       1.000000       1.000000   \n",
      "\n",
      "             f778_59        f778_60        f778_61        f778_62  \\\n",
      "count  105471.000000  105471.000000  105471.000000  105471.000000   \n",
      "mean        0.040172       0.054517       0.053266       0.022357   \n",
      "std         0.196364       0.227037       0.224564       0.147842   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "             f778_63             id           loss  \n",
      "count  105471.000000  105471.000000  105471.000000  \n",
      "mean        0.001195   52736.000000       0.799585  \n",
      "std         0.034543   30446.999458       4.321120  \n",
      "min         0.000000       1.000000       0.000000  \n",
      "25%         0.000000   26368.500000       0.000000  \n",
      "50%         0.000000   52736.000000       0.000000  \n",
      "75%         0.000000   79103.500000       0.000000  \n",
      "max         1.000000  105471.000000     100.000000  \n",
      "\n",
      "[8 rows x 3905 columns]\n"
     ]
    }
   ],
   "source": [
    "# Expand continuous features to be polynomial (degree 3)\n",
    "\n",
    "feature_def = gen_features(\n",
    "    columns=cols_continuous,\n",
    "    classes=[{'class': PolynomialFeatures, 'degree': 3}]\n",
    ")\n",
    "\n",
    "mapper = DataFrameMapper(feature_def, default=None, input_df=True, df_out=True)\n",
    "df_poly3 = mapper.fit_transform(df_imputed_normalized)\n",
    "df_poly3.info()\n",
    "print(df_poly3.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the expanded DataFrame into a new CSV\n",
    "\n",
    "df_poly3.to_csv('train_v2_preprocessed_normalized_poly3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
